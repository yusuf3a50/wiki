{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294cf16e",
   "metadata": {},
   "source": [
    "# Parquet Files with Python\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Create Parquet database files\n",
    "- Query data from Parquet files\n",
    "- Update and append data to Parquet files\n",
    "- Use pandas, pyarrow, and DuckDB for efficient Parquet operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631cb246",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install required libraries:\n",
    "```bash\n",
    "pip install pandas pyarrow fastparquet duckdb\n",
    "```\n",
    "\n",
    "**Libraries:**\n",
    "- `pandas` - Data manipulation and analysis\n",
    "- `pyarrow` - Apache Arrow implementation for reading/writing Parquet\n",
    "- `fastparquet` - Alternative Parquet implementation\n",
    "- `duckdb` - SQL queries on Parquet files without loading into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384e38cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n",
      "Pandas version: 2.3.3\n",
      "PyArrow version: 22.0.0\n",
      "DuckDB version: 1.4.3\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"PyArrow version: {pa.__version__}\")\n",
    "print(f\"DuckDB version: {duckdb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d230406a",
   "metadata": {},
   "source": [
    "## 1. Create Sample Data\n",
    "\n",
    "First, let's create sample data to work with. We'll create a dataset of employees with various attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4609ad7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Employee Data:\n",
      "   employee_id           name   department  salary  years_experience  remote\n",
      "0            1  Alice Johnson  Engineering   95000                 5    True\n",
      "1            2      Bob Smith        Sales   65000                 3   False\n",
      "2            3  Charlie Brown  Engineering   88000                 4    True\n",
      "3            4   Diana Prince           HR   72000                 6   False\n",
      "4            5      Eve Davis        Sales   70000                 2    True\n",
      "5            6   Frank Miller  Engineering  105000                 8    True\n",
      "6            7      Grace Lee           HR   68000                 4   False\n",
      "7            8   Henry Wilson        Sales   75000                 5   False\n",
      "8            9      Iris Chen  Engineering   92000                 6    True\n",
      "9           10    Jack Taylor           HR   71000                 3   False\n",
      "\n",
      "Shape: (10, 6)\n",
      "Columns: ['employee_id', 'name', 'department', 'salary', 'years_experience', 'remote']\n"
     ]
    }
   ],
   "source": [
    "# Create sample employee data\n",
    "data = {\n",
    "    'employee_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'name': ['Alice Johnson', 'Bob Smith', 'Charlie Brown', 'Diana Prince', \n",
    "             'Eve Davis', 'Frank Miller', 'Grace Lee', 'Henry Wilson', \n",
    "             'Iris Chen', 'Jack Taylor'],\n",
    "    'department': ['Engineering', 'Sales', 'Engineering', 'HR', 'Sales', \n",
    "                   'Engineering', 'HR', 'Sales', 'Engineering', 'HR'],\n",
    "    'salary': [95000, 65000, 88000, 72000, 70000, 105000, 68000, 75000, 92000, 71000],\n",
    "    'years_experience': [5, 3, 4, 6, 2, 8, 4, 5, 6, 3],\n",
    "    'remote': [True, False, True, False, True, True, False, False, True, False]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the data\n",
    "print(\"Sample Employee Data:\")\n",
    "print(df)\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4dec4",
   "metadata": {},
   "source": [
    "## 2. Write Data to Parquet File\n",
    "\n",
    "Parquet is a columnar storage format that provides efficient compression and encoding. It's ideal for analytical queries and big data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83be528c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data written to employees.parquet\n",
      "File size: 4,219 bytes (4.12 KB)\n",
      "‚úÖ Also created employees_pyarrow.parquet using PyArrow directly\n"
     ]
    }
   ],
   "source": [
    "# Define file path\n",
    "parquet_file = 'employees.parquet'\n",
    "\n",
    "# Method 1: Using pandas (simple and common)\n",
    "df.to_parquet(parquet_file, engine='pyarrow', compression='snappy', index=False)\n",
    "print(f\"‚úÖ Data written to {parquet_file}\")\n",
    "\n",
    "# Check file size\n",
    "file_size = Path(parquet_file).stat().st_size\n",
    "print(f\"File size: {file_size:,} bytes ({file_size/1024:.2f} KB)\")\n",
    "\n",
    "# Method 2: Using PyArrow directly (more control)\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table, 'employees_pyarrow.parquet', compression='snappy')\n",
    "print(\"‚úÖ Also created employees_pyarrow.parquet using PyArrow directly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e337e16",
   "metadata": {},
   "source": [
    "## 3. Read Data from Parquet File\n",
    "\n",
    "Reading Parquet files is straightforward and efficient, especially for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4389cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Read entire file with pandas\n",
    "df_read = pd.read_parquet(parquet_file, engine='pyarrow')\n",
    "print(\"Data read from Parquet file:\")\n",
    "print(df_read)\n",
    "\n",
    "# Method 2: Read only specific columns (efficient!)\n",
    "df_subset = pd.read_parquet(parquet_file, columns=['name', 'department', 'salary'])\n",
    "print(\"\\nüìå Reading only specific columns:\")\n",
    "print(df_subset.head())\n",
    "\n",
    "# Method 3: Using PyArrow for more control\n",
    "table = pq.read_table(parquet_file)\n",
    "print(f\"\\nüìä Schema information:\")\n",
    "print(table.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0b3a3",
   "metadata": {},
   "source": [
    "## 4. Query Data with Pandas\n",
    "\n",
    "Use pandas filtering and query methods to select specific data from Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be15ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df = pd.read_parquet(parquet_file)\n",
    "\n",
    "# Query 1: Filter by department\n",
    "engineering = df[df['department'] == 'Engineering']\n",
    "print(\"üîç Query 1: Engineering employees\")\n",
    "print(engineering[['name', 'department', 'salary']])\n",
    "\n",
    "# Query 2: Filter by salary range\n",
    "high_earners = df[df['salary'] >= 90000]\n",
    "print(\"\\nüîç Query 2: Employees earning >= $90,000\")\n",
    "print(high_earners[['name', 'salary', 'department']])\n",
    "\n",
    "# Query 3: Multiple conditions\n",
    "experienced_remote = df[(df['years_experience'] >= 5) & (df['remote'] == True)]\n",
    "print(\"\\nüîç Query 3: Remote employees with 5+ years experience\")\n",
    "print(experienced_remote[['name', 'years_experience', 'remote']])\n",
    "\n",
    "# Query 4: Using .query() method (SQL-like syntax)\n",
    "sales_dept = df.query(\"department == 'Sales' and salary > 65000\")\n",
    "print(\"\\nüîç Query 4: Sales employees earning > $65,000\")\n",
    "print(sales_dept[['name', 'salary']])\n",
    "\n",
    "# Query 5: Aggregations\n",
    "print(\"\\nüìä Query 5: Average salary by department\")\n",
    "print(df.groupby('department')['salary'].agg(['mean', 'count', 'min', 'max']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359d2865",
   "metadata": {},
   "source": [
    "## 5. Query Data with SQL (DuckDB)\n",
    "\n",
    "DuckDB allows you to run SQL queries directly on Parquet files without loading them entirely into memory. This is extremely efficient for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DuckDB connection\n",
    "conn = duckdb.connect(':memory:')  # In-memory database\n",
    "\n",
    "# SQL Query 1: SELECT all data\n",
    "result = conn.execute(f\"SELECT * FROM '{parquet_file}'\").df()\n",
    "print(\"üîç SQL Query 1: SELECT all rows\")\n",
    "print(result)\n",
    "\n",
    "# SQL Query 2: Filter by department\n",
    "query2 = f\"\"\"\n",
    "    SELECT name, department, salary \n",
    "    FROM '{parquet_file}' \n",
    "    WHERE department = 'Engineering'\n",
    "    ORDER BY salary DESC\n",
    "\"\"\"\n",
    "result2 = conn.execute(query2).df()\n",
    "print(\"\\nüîç SQL Query 2: Engineering department (sorted by salary)\")\n",
    "print(result2)\n",
    "\n",
    "# SQL Query 3: Aggregations with GROUP BY\n",
    "query3 = f\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as employee_count,\n",
    "        AVG(salary) as avg_salary,\n",
    "        MAX(salary) as max_salary\n",
    "    FROM '{parquet_file}'\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\"\n",
    "result3 = conn.execute(query3).df()\n",
    "print(\"\\nüîç SQL Query 3: Department statistics\")\n",
    "print(result3)\n",
    "\n",
    "# SQL Query 4: Complex query with multiple conditions\n",
    "query4 = f\"\"\"\n",
    "    SELECT name, salary, years_experience, remote\n",
    "    FROM '{parquet_file}'\n",
    "    WHERE salary > 70000 \n",
    "      AND (remote = true OR years_experience >= 5)\n",
    "    ORDER BY salary DESC\n",
    "\"\"\"\n",
    "result4 = conn.execute(query4).df()\n",
    "print(\"\\nüîç SQL Query 4: High earners who are remote or experienced\")\n",
    "print(result4)\n",
    "\n",
    "# SQL Query 5: JOIN example (self-join to compare salaries)\n",
    "query5 = f\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(CASE WHEN salary > 80000 THEN 1 END) as high_earners,\n",
    "        COUNT(*) as total_employees\n",
    "    FROM '{parquet_file}'\n",
    "    GROUP BY department\n",
    "\"\"\"\n",
    "result5 = conn.execute(query5).df()\n",
    "print(\"\\nüîç SQL Query 5: High earners (>$80k) by department\")\n",
    "print(result5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0e757e",
   "metadata": {},
   "source": [
    "## 6. Update Data in Parquet File\n",
    "\n",
    "Parquet files are immutable (read-only), so \"updating\" means reading the file, modifying the data in memory, and writing it back to the same file (or a new file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb9f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read existing data\n",
    "df = pd.read_parquet(parquet_file)\n",
    "print(\"Original data:\")\n",
    "print(df[['employee_id', 'name', 'salary']])\n",
    "\n",
    "# Update 1: Give everyone in Engineering a 10% raise\n",
    "df.loc[df['department'] == 'Engineering', 'salary'] = \\\n",
    "    df.loc[df['department'] == 'Engineering', 'salary'] * 1.10\n",
    "\n",
    "# Update 2: Update specific employee by ID\n",
    "df.loc[df['employee_id'] == 5, 'years_experience'] = 3  # Eve got promoted\n",
    "\n",
    "# Update 3: Add a new column\n",
    "df['bonus'] = df['salary'] * 0.10  # 10% bonus for all\n",
    "\n",
    "print(\"\\n‚úÖ Updated data:\")\n",
    "print(df[['employee_id', 'name', 'salary', 'bonus']])\n",
    "\n",
    "# Write updated data back to file\n",
    "df.to_parquet(parquet_file, engine='pyarrow', compression='snappy', index=False)\n",
    "print(f\"\\n‚úÖ Updated data written back to {parquet_file}\")\n",
    "\n",
    "# Verify the update\n",
    "df_verify = pd.read_parquet(parquet_file)\n",
    "print(\"\\nEngineering salaries after 10% raise:\")\n",
    "print(df_verify[df_verify['department'] == 'Engineering'][['name', 'salary']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7818f",
   "metadata": {},
   "source": [
    "## 7. Append New Data to Parquet File\n",
    "\n",
    "Add new rows to an existing Parquet file by reading it, concatenating new data, and writing back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new employee records\n",
    "new_employees = pd.DataFrame({\n",
    "    'employee_id': [11, 12, 13],\n",
    "    'name': ['Karen White', 'Leo Martinez', 'Maya Patel'],\n",
    "    'department': ['Engineering', 'Sales', 'HR'],\n",
    "    'salary': [98000, 72000, 74000],\n",
    "    'years_experience': [7, 4, 5],\n",
    "    'remote': [True, False, True],\n",
    "    'bonus': [9800, 7200, 7400]  # Include the bonus column we added\n",
    "})\n",
    "\n",
    "print(\"New employees to add:\")\n",
    "print(new_employees)\n",
    "\n",
    "# Read existing data\n",
    "df_existing = pd.read_parquet(parquet_file)\n",
    "print(f\"\\nCurrent number of employees: {len(df_existing)}\")\n",
    "\n",
    "# Append new data\n",
    "df_combined = pd.concat([df_existing, new_employees], ignore_index=True)\n",
    "print(f\"After append: {len(df_combined)} employees\")\n",
    "\n",
    "# Write combined data back\n",
    "df_combined.to_parquet(parquet_file, engine='pyarrow', compression='snappy', index=False)\n",
    "print(f\"‚úÖ New employees added to {parquet_file}\")\n",
    "\n",
    "# Verify\n",
    "df_verify = pd.read_parquet(parquet_file)\n",
    "print(\"\\nAll employees (showing last 5):\")\n",
    "print(df_verify.tail()[['employee_id', 'name', 'department', 'salary']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c8639b",
   "metadata": {},
   "source": [
    "## 8. Delete Data from Parquet File\n",
    "\n",
    "Remove specific rows by filtering them out and writing the remaining data back to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769d88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read current data\n",
    "df = pd.read_parquet(parquet_file)\n",
    "print(f\"Current employee count: {len(df)}\")\n",
    "print(\"\\nCurrent employees:\")\n",
    "print(df[['employee_id', 'name', 'department']])\n",
    "\n",
    "# Delete 1: Remove specific employee by ID\n",
    "df_filtered = df[df['employee_id'] != 12]  # Remove Leo Martinez (ID 12)\n",
    "\n",
    "# Delete 2: Remove all employees from a specific department\n",
    "# df_filtered = df_filtered[df_filtered['department'] != 'HR']  # Uncomment to remove HR\n",
    "\n",
    "print(f\"\\nAfter deletion: {len(df_filtered)} employees\")\n",
    "print(df_filtered[['employee_id', 'name', 'department']])\n",
    "\n",
    "# Write back to file\n",
    "df_filtered.to_parquet(parquet_file, engine='pyarrow', compression='snappy', index=False)\n",
    "print(f\"\\n‚úÖ Employee(s) removed from {parquet_file}\")\n",
    "\n",
    "# Verify deletion\n",
    "df_verify = pd.read_parquet(parquet_file)\n",
    "print(f\"\\nVerified: {len(df_verify)} employees remain\")\n",
    "print(\"Employee IDs:\", df_verify['employee_id'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9660a99",
   "metadata": {},
   "source": [
    "## 9. Partitioned Parquet Files (Advanced)\n",
    "\n",
    "For large datasets, you can partition Parquet files by column values (e.g., by department). This creates separate files for each partition, enabling faster queries when filtering by the partition column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d812b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_parquet(parquet_file)\n",
    "\n",
    "# Write partitioned by department\n",
    "partition_dir = 'employees_partitioned'\n",
    "df.to_parquet(\n",
    "    partition_dir,\n",
    "    engine='pyarrow',\n",
    "    partition_cols=['department'],  # Creates separate files per department\n",
    "    compression='snappy',\n",
    "    index=False\n",
    ")\n",
    "print(f\"‚úÖ Created partitioned Parquet files in '{partition_dir}/' directory\")\n",
    "\n",
    "# List partition directories\n",
    "import os\n",
    "if os.path.exists(partition_dir):\n",
    "    for item in os.listdir(partition_dir):\n",
    "        print(f\"  üìÅ {item}\")\n",
    "\n",
    "# Read from partitioned dataset - only reads relevant partitions\n",
    "df_engineering = pd.read_parquet(\n",
    "    partition_dir,\n",
    "    filters=[('department', '==', 'Engineering')]  # Only reads Engineering partition\n",
    ")\n",
    "print(f\"\\nüîç Read only Engineering partition: {len(df_engineering)} rows\")\n",
    "print(df_engineering[['name', 'salary']])\n",
    "\n",
    "# Read entire partitioned dataset\n",
    "df_all = pd.read_parquet(partition_dir)\n",
    "print(f\"\\nüìä Total rows from all partitions: {len(df_all)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da85d2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "‚úÖ **Create Parquet files** - Using pandas with PyArrow backend\n",
    "‚úÖ **Read Parquet files** - Full reads, column selection, and schema inspection\n",
    "‚úÖ **Query with Pandas** - Filtering, aggregations, and the `.query()` method\n",
    "‚úÖ **Query with SQL** - Using DuckDB for SQL queries directly on Parquet files\n",
    "‚úÖ **Update data** - Modify values and add columns\n",
    "‚úÖ **Append data** - Add new rows to existing files\n",
    "‚úÖ **Delete data** - Remove rows by filtering\n",
    "‚úÖ **Partition data** - Create partitioned datasets for efficient querying\n",
    "\n",
    "### Key Advantages of Parquet:\n",
    "- **Columnar storage** - Only read columns you need\n",
    "- **Compression** - Smaller file sizes than CSV\n",
    "- **Type preservation** - Data types are stored, no parsing needed\n",
    "- **Fast queries** - Especially with DuckDB or partitioning\n",
    "- **Schema evolution** - Can add/remove columns over time\n",
    "\n",
    "### Best Practices:\n",
    "1. Use `compression='snappy'` for good balance of speed and size\n",
    "2. Partition large datasets by frequently-filtered columns\n",
    "3. Use DuckDB for SQL queries on large Parquet files (avoids loading into memory)\n",
    "4. Read only necessary columns with `columns=['col1', 'col2']`\n",
    "5. For production, consider using a proper database or data lake\n",
    "\n",
    "### Next Steps:\n",
    "- Integrate with Apache Spark for very large datasets\n",
    "- Use with cloud storage (S3, Azure Blob, GCS)\n",
    "- Implement incremental updates with Delta Lake or Apache Iceberg\n",
    "- Create data pipelines with Parquet as the storage format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
